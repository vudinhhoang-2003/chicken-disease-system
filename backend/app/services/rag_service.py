import logging
from typing import List, Optional, Dict
import chromadb
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.schema import SystemMessage, HumanMessage
from sqlalchemy.orm import joinedload

from app.config import get_settings
from app.core import models
from app.core.database import SessionLocal

logger = logging.getLogger(__name__)
settings = get_settings()

class RAGService:
    """Service for RAG using Local HuggingFace Embeddings and Google Gemini LLM"""

    def __init__(self):
        self.settings = get_settings()
        
        # 1. Initialize Gemini LLM (Optional fallback)
        if self.settings.google_api_key:
            self.llm = ChatGoogleGenerativeAI(
                model="gemini-1.5-flash",
                google_api_key=self.settings.google_api_key,
                temperature=0.2
            )
        else:
            logger.warning("âš ï¸ GOOGLE_API_KEY not found. LLM will not be available.")
            self.llm = None

        # 2. Initialize LOCAL Embeddings (HuggingFace)
        try:
            logger.info("ðŸ“¡ Loading Local Embedding Model (paraphrase-multilingual-MiniLM-L12-v2)...")
            self.embeddings = HuggingFaceEmbeddings(
                model_name="paraphrase-multilingual-MiniLM-L12-v2"
            )
            logger.info("âœ… Local Embedding Model loaded.")
        except Exception as e:
            logger.error(f"âŒ Failed to load Embedding model: {e}")
            self.embeddings = None

        # 3. Initialize ChromaDB Client
        try:
            self.chroma_client = chromadb.HttpClient(
                host=self.settings.chroma_host,
                port=self.settings.chroma_port,
                settings=chromadb.Settings(
                    allow_reset=True,
                    anonymized_telemetry=False
                )
            )
            # Ensure heartbeat to check connection
            self.chroma_client.heartbeat()
            self.collection = self.chroma_client.get_or_create_collection(name="chicken_knowledge")
            logger.info("âœ… Connected to ChromaDB")
        except Exception as e:
            logger.error(f"âŒ Failed to connect to ChromaDB: {e}")
            self.chroma_client = None

    def _format_disease_text(self, disease: models.Disease) -> str:
        """Helper to format disease info into a structured document"""
        text = f"Bá»†NH: {disease.name_vi} ({disease.name_en})\n"
        if disease.source:
            text += f"NGUá»’N TÃ€I LIá»†U: {disease.source}\n"
        text += f"MÃƒ Bá»†NH: {disease.code}\n\n"
        text += f"TRIá»†U CHá»¨NG:\n{disease.symptoms}\n\n"
        text += f"NGUYÃŠN NHÃ‚N:\n{disease.cause}\n\n"
        text += f"PHÃ’NG Bá»†NH:\n{disease.prevention}\n\n"
        
        if disease.treatment_steps:
            text += "PHÃC Äá»’ ÄIá»€U TRá»Š:\n"
            steps = sorted(disease.treatment_steps, key=lambda x: x.step_order)
            for step in steps:
                text += f"- BÆ°á»›c {step.step_order}: {step.description}\n"
                if step.action:
                    text += f"  -> HÃ nh Ä‘á»™ng: {step.action}\n"
                for med in step.medicines:
                    text += f"  -> Thuá»‘c: {med.name} (Liá»u: {med.dosage})\n"
        return text

    def sync_disease(self, disease_id: int):
        """Sync a disease from SQL to Vector DB using Local Embeddings"""
        if not self.chroma_client or not self.embeddings:
            logger.error("ChromaDB or Embeddings not initialized")
            return

        db = SessionLocal()
        try:
            disease = db.query(models.Disease).options(
                joinedload(models.Disease.treatment_steps).joinedload(models.TreatmentStep.medicines)
            ).filter(models.Disease.id == disease_id).first()
            
            if not disease:
                return

            text_content = self._format_disease_text(disease)
            
            # Using langchain embeddings to generate vector
            vector = self.embeddings.embed_query(text_content)
            
            # Upsert into ChromaDB
            self.collection.upsert(
                ids=[str(disease.id)],
                embeddings=[vector],
                documents=[text_content],
                metadatas=[{
                    "id": disease.id,
                    "code": disease.code,
                    "name": disease.name_vi,
                    "source": disease.source or "ChÆ°a rÃµ"
                }]
            )
            logger.info(f"âœ¨ Synced {disease.name_vi} to Vector DB (Local)")
        except Exception as e:
            logger.error(f"âŒ Sync error: {e}")
        finally:
            db.close()

    def delete_disease_vector(self, disease_id: int):
        """Remove a disease from Vector DB"""
        if self.collection:
            try:
                self.collection.delete(ids=[str(disease_id)])
                logger.info(f"ðŸ—‘ï¸ Deleted disease ID {disease_id} from Vector DB")
            except Exception as e:
                logger.error(f"âŒ Delete vector error: {e}")

    async def answer_question(self, question: str, history: List[Dict] = []) -> str:
        """Answer question using Local Semantic Search + Gemini (if available)"""
        if not self.chroma_client or not self.embeddings:
            return "Xin lá»—i, há»‡ thá»‘ng AI hiá»‡n chÆ°a sáºµn sÃ ng."

        try:
            # 1. Search semantic context using local embeddings
            query_vector = self.embeddings.embed_query(question)
            results = self.collection.query(
                query_embeddings=[query_vector],
                n_results=2
            )
            
            context = ""
            if results['documents'] and results['documents'][0]:
                context = "THÃ”NG TIN CHUYÃŠN MÃ”N TÃŒM THáº¤Y:\n\n" + "\n---\n".join(results['documents'][0])
            
            # 2. Fallback if no Gemini Key
            if not self.llm:
                if context:
                    return f"TÃ´i Ä‘Ã£ tÃ¬m tháº¥y thÃ´ng tin sau cho báº¡n:\n\n{context}\n\n(LÆ°u Ã½: TÃ´i Ä‘ang cháº¡y á»Ÿ cháº¿ Ä‘á»™ tÃ¬m kiáº¿m trá»±c tiáº¿p vÃ¬ chÆ°a cÃ³ API Key cho Chatbot)."
                return "Xin lá»—i, tÃ´i chÆ°a tÃ¬m tháº¥y kiáº¿n thá»©c nÃ o khá»›p vá»›i cÃ¢u há»i cá»§a báº¡n."

            # 3. Use Gemini to format answer
            system_prompt = f"""
            Báº¡n lÃ  chuyÃªn gia ThÃº y AI. Tráº£ lá»i dá»±a trÃªn ngá»¯ cáº£nh dÆ°á»›i Ä‘Ã¢y:
            
            {context}
            
            Náº¿u khÃ´ng cÃ³ thÃ´ng tin, hÃ£y tráº£ lá»i theo hiá»ƒu biáº¿t chuyÃªn mÃ´n vÃ  nháº¯c ngÆ°á»i dÃ¢n cáº©n trá»ng.
            LuÃ´n nÃªu NGUá»’N TÃ€I LIá»†U náº¿u cÃ³.
            """
            
            messages = [SystemMessage(content=system_prompt)]
            for msg in history[-5:]:
                messages.append(HumanMessage(content=msg["content"]) if msg["role"] == "user" else SystemMessage(content=msg["content"]))
            messages.append(HumanMessage(content=question))
            
            response = await self.llm.ainvoke(messages)
            return response.content
            
        except Exception as e:
            logger.error(f"âŒ RAG Error: {e}")
            return "ÄÃ£ xáº£y ra lá»—i khi xá»­ lÃ½ cÃ¢u há»i cá»§a báº¡n."

# Singleton
_rag_service: Optional[RAGService] = None

def get_rag_service() -> RAGService:
    global _rag_service
    if _rag_service is None:
        _rag_service = RAGService()
    return _rag_service